---
title: "Clustering Methods"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Clustering Methods}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.height = 5
)
```

```{r setup}
library(tidyul)
library(dplyr)
library(ggplot2)
```

## Introduction

Clustering groups similar observations together without predefined labels. This vignette covers:

- K-means clustering
- Partitioning Around Medoids (PAM) and CLARA
- Hierarchical clustering
- Density-based clustering (DBSCAN)
- Cluster validation and comparison
- Choosing the right method

## K-Means Clustering

K-means partitions data into k clusters by minimizing within-cluster variance.

### Basic K-Means

```{r basic-kmeans}
# Prepare iris data
iris_data <- iris %>% select(-Species)

# Apply k-means with k=3
km_result <- tidy_kmeans(iris_data, k = 3, nstart = 25)

# View results
names(km_result)

# Cluster centers
km_result$centers

# Cluster sizes
km_result$size
```

### Visualizing Clusters

```{r plot-kmeans}
# Augment original data
iris_clustered <- augment_kmeans(km_result, iris_data) %>%
  mutate(species = iris$Species)

# Built-in plot
plot_clusters(iris_clustered)

# Custom plot
iris_clustered %>%
  ggplot(aes(x = Sepal.Length, y = Sepal.Width, color = factor(cluster))) +
  geom_point(size = 3, alpha = 0.7) +
  geom_point(data = km_result$centers, aes(x = Sepal.Length, y = Sepal.Width),
             color = "black", size = 5, shape = 4, stroke = 2) +
  labs(color = "Cluster", title = "K-Means Clustering (k=3)") +
  theme_minimal()
```

### Determining Optimal K

#### Elbow Method

```{r elbow-method}
# Calculate WSS for different k values
wss_values <- calc_wss(iris_data, max_k = 10)

# Plot elbow curve
plot_elbow(wss_values)

# Look for the "elbow" where improvement slows
```

#### Silhouette Analysis

```{r silhouette-analysis}
# Analyze silhouettes for different k values
sil_analysis <- tidy_silhouette_analysis(iris_data, max_k = 6)

sil_analysis

# Plot average silhouette width
sil_analysis %>%
  ggplot(aes(x = k, y = avg_sil_width)) +
  geom_line(size = 1) +
  geom_point(size = 3) +
  labs(title = "Average Silhouette Width by k",
       y = "Average Silhouette Width",
       x = "Number of Clusters (k)") +
  theme_minimal()
```

#### Gap Statistic

```{r gap-statistic}
# Calculate gap statistic (may take a moment)
gap_result <- tidy_gap_stat(iris_data, max_k = 6, B = 50)

# Plot
plot_gap_stat(gap_result)

# Optimal k
gap_result$optimal_k
```

### Practical Tips for K-Means

```{r kmeans-tips}
# Always use nstart > 1 to avoid local minima
km_stable <- tidy_kmeans(iris_data, k = 3, nstart = 50)

# Standardize data if variables have different scales
iris_std <- standardize_data(iris_data)
km_std <- tidy_kmeans(iris_std, k = 3, nstart = 25)

# Compare results
table(
  Original = augment_kmeans(km_result, iris_data)$cluster,
  Standardized = augment_kmeans(km_std, iris_std)$cluster
)
```

## PAM (Partitioning Around Medoids)

PAM is more robust to outliers than k-means, using actual data points as cluster centers.

### Basic PAM

```{r basic-pam}
# Apply PAM
pam_result <- tidy_pam(iris_data, k = 3)

# View medoids
pam_result$medoids

# Plot
pam_augmented <- augment_pam(pam_result, iris_data)
plot_clusters(pam_augmented)
```

### PAM vs K-Means

```{r pam-vs-kmeans}
# Compare with k-means
km_clusters <- augment_kmeans(km_result, iris_data)$cluster
pam_clusters <- augment_pam(pam_result, iris_data)$cluster

# Agreement between methods
table(KMeans = km_clusters, PAM = pam_clusters)

# Calculate adjusted Rand index
# library(mclust)
# adjustedRandIndex(km_clusters, pam_clusters)
```

## CLARA (Clustering Large Applications)

CLARA extends PAM to large datasets using sampling.

```{r clara}
# Create larger dataset
set.seed(123)
large_list <- create_example_data(n = 1000, k = 4, p = 5)
large_data <- large_list$data

# CLARA is efficient for large data
clara_result <- tidy_clara(large_data, k = 4, samples = 50, sampsize = 100)

# View results
clara_result$size

# Plot (sample for visualization)
clara_augmented <- cbind(large_data, cluster = clara_result$clusters$cluster)
plot_clusters(clara_augmented)
```

## Hierarchical Clustering

Hierarchical clustering creates a tree (dendrogram) of nested clusters.

### Basic Hierarchical Clustering

```{r basic-hclust}
# Perform hierarchical clustering
hc_result <- tidy_hclust(iris_data, method = "complete")

# View dendrogram
plot_dendrogram(hc_result)
```

### Different Linkage Methods

```{r linkage-methods}
# Complete linkage (default)
hc_complete <- tidy_hclust(iris_data, method = "complete")

# Average linkage
hc_average <- tidy_hclust(iris_data, method = "average")

# Single linkage
hc_single <- tidy_hclust(iris_data, method = "single")

# Ward's method (minimizes within-cluster variance)
hc_ward <- tidy_hclust(iris_data, method = "ward.D2")

# Compare dendrograms
plot_dendrogram(hc_ward, title = "Hierarchical Clustering (Ward's Method)")
```

### Cutting the Dendrogram

```{r cut-dendrogram}
# Cut at k=3 clusters
clusters <- tidy_cutree(hc_result, k = 3)

head(clusters)

# Add clusters to data manually
iris_hc <- iris_data %>%
  mutate(cluster = clusters$cluster)

# Visualize
iris_hc %>%
  ggplot(aes(x = Sepal.Length, y = Sepal.Width, color = factor(cluster))) +
  geom_point(size = 3, alpha = 0.7) +
  labs(color = "Cluster", title = "Hierarchical Clustering (k=3)") +
  theme_minimal()
```

### Determining Optimal K

```{r hclust-optimal-k}
# Find optimal number of clusters
optimal <- optimal_hclust_k(hc_result, max_k = 10)

optimal
```

## DBSCAN (Density-Based Clustering)

DBSCAN identifies clusters based on density and can find arbitrary-shaped clusters and noise points.

### Basic DBSCAN

```{r basic-dbscan}
# Create data with clear clusters and noise
set.seed(123)
cluster_data <- data.frame(
  x = c(rnorm(50, 0, 0.3), rnorm(50, 3, 0.3), runif(10, -2, 5)),
  y = c(rnorm(50, 0, 0.3), rnorm(50, 3, 0.3), runif(10, -2, 5))
)

# Apply DBSCAN
db_result <- tidy_dbscan(cluster_data, eps = 0.5, minPts = 5)

# View results (cluster 0 = noise)
table(db_result$clusters$cluster)

# Plot
augment_dbscan(db_result, cluster_data) %>%
  ggplot(aes(x = x, y = y, color = factor(cluster))) +
  geom_point(size = 3, alpha = 0.7) +
  labs(color = "Cluster", title = "DBSCAN Clustering (0 = Noise)") +
  scale_color_manual(values = c("0" = "gray50", "1" = "red", "2" = "blue")) +
  theme_minimal()
```

### Choosing eps Parameter

```{r choose-eps}
# Calculate k-NN distances
knn_dist <- tidy_knn_dist(cluster_data, k = 5)

# Plot k-NN distance
plot_knn_dist(knn_dist) +
  labs(title = "k-NN Distance Plot (k=5)",
       subtitle = "Look for 'knee' to determine eps")

# Get suggested eps from knn_dist object
suggested_eps <- suggest_eps(knn_dist)
suggested_eps
```

### Parameter Exploration

```{r dbscan-params}
# Explore different parameter combinations manually
eps_vals <- seq(0.3, 0.8, by = 0.1)
minPts_vals <- seq(3, 10, by = 1)

param_results <- expand.grid(eps = eps_vals, minPts = minPts_vals) %>%
  rowwise() %>%
  mutate(
    n_clusters = {
      result <- tidy_dbscan(cluster_data, eps = eps, minPts = minPts)
      max(result$clusters$cluster)
    }
  ) %>%
  ungroup()

param_results %>%
  ggplot(aes(x = eps, y = minPts, fill = n_clusters)) +
  geom_tile() +
  geom_text(aes(label = n_clusters), color = "white") +
  scale_fill_gradient(low = "blue", high = "red") +
  labs(title = "DBSCAN Parameter Exploration",
       fill = "# Clusters") +
  theme_minimal()
```

## Comparing Clustering Methods

### Validation Metrics

```{r validation-metrics}
# Apply different methods
km3 <- tidy_kmeans(iris_data, k = 3, nstart = 25)
pam3 <- tidy_pam(iris_data, k = 3)
hc3 <- tidy_hclust(iris_data)

# Calculate distance matrix for silhouette analysis
iris_dist <- dist(iris_data)

# Get cluster assignments (convert to numeric)
km_clusters <- as.numeric(as.character(augment_kmeans(km3, iris_data)$cluster))
pam_clusters <- as.numeric(as.character(augment_pam(pam3, iris_data)$cluster))

# Calculate silhouette for validation
km_sil <- tidy_silhouette(km_clusters, iris_dist)
pam_sil <- tidy_silhouette(pam_clusters, iris_dist)

# Compare average silhouette widths
km_sil$avg_width
pam_sil$avg_width
```

### Side-by-Side Comparison

```{r compare-methods}
# Compare silhouette widths
comparison_df <- data.frame(
  Method = c("K-Means", "PAM"),
  Avg_Silhouette = c(km_sil$avg_width, pam_sil$avg_width),
  N_Clusters = c(3, 3)
)

comparison_df

# Visualize comparison
comparison_df %>%
  ggplot(aes(x = Method, y = Avg_Silhouette, fill = Method)) +
  geom_col() +
  geom_text(aes(label = round(Avg_Silhouette, 3)), vjust = -0.5) +
  labs(title = "Clustering Method Comparison",
       y = "Average Silhouette Width") +
  theme_minimal() +
  theme(legend.position = "none")
```

### Cluster Size Distribution

```{r cluster-sizes}
# Compare cluster sizes
# Create manual cluster size plots
km_size_df <- data.frame(cluster = km_clusters) %>%
  count(cluster)

km_size_df %>%
  ggplot(aes(x = factor(cluster), y = n, fill = factor(cluster))) +
  geom_col() +
  geom_text(aes(label = n), vjust = -0.5) +
  labs(title = "K-Means Cluster Sizes", x = "Cluster", y = "Count") +
  theme_minimal() +
  theme(legend.position = "none")

pam_size_df <- data.frame(cluster = pam_clusters) %>%
  count(cluster)

pam_size_df %>%
  ggplot(aes(x = factor(cluster), y = n, fill = factor(cluster))) +
  geom_col() +
  geom_text(aes(label = n), vjust = -0.5) +
  labs(title = "PAM Cluster Sizes", x = "Cluster", y = "Count") +
  theme_minimal() +
  theme(legend.position = "none")
```

## Choosing the Right Method

| Method | Pros | Cons | Best For |
|--------|------|------|----------|
| **K-Means** | Fast, simple, scalable | Requires k, sensitive to outliers, spherical clusters | Large datasets, well-separated spherical clusters |
| **PAM** | Robust to outliers, interpretable medoids | Slower than k-means, requires k | Moderate datasets with outliers |
| **CLARA** | Scales PAM to large data | Requires k, sampling may miss patterns | Large datasets needing robustness |
| **Hierarchical** | No k needed upfront, creates hierarchy | Slow for large data, hard to interpret | Understanding cluster relationships |
| **DBSCAN** | Finds arbitrary shapes, identifies noise | Requires eps/minPts tuning, struggles with varying densities | Non-spherical clusters, noisy data |

## Practical Workflow

```{r workflow}
# 1. Explore your data
summary(iris_data)

# 2. Standardize if needed
iris_std <- standardize_data(iris_data)

# 3. Quick clustering analysis
quick_result <- quick_cluster(iris_std, max_k = 6)
quick_result$recommendation

# 4. Apply chosen method
final_clustering <- tidy_kmeans(iris_std, k = 3, nstart = 50)

# 5. Validate
iris_std_dist <- dist(iris_std)
final_clusters <- as.numeric(as.character(augment_kmeans(final_clustering, iris_std)$cluster))
sil_result <- tidy_silhouette(final_clusters, iris_std_dist)
plot_silhouette(sil_result)

# 6. Interpret and export
final_data <- augment_kmeans(final_clustering, iris_data) %>%
  mutate(species = iris$Species)

# Compare with known labels
table(Cluster = final_data$cluster, Species = final_data$species)
```

## Advanced: Creating Dashboards

```{r dashboard, eval=FALSE}
# Create comprehensive clustering dashboard
dashboard <- create_cluster_dashboard(final_clustering, iris_std)

# View all plots
dashboard
```

## Summary

**Key Takeaways:**

1. **Always standardize** data unless variables are on same scale
2. **Use multiple methods** to determine optimal k
3. **Validate results** with silhouette analysis or gap statistic
4. **Compare methods** when unsure which to use
5. **Interpret in context** - relate clusters back to domain knowledge

**Function Quick Reference:**

- `tidy_kmeans()` - K-means clustering
- `tidy_pam()` / `tidy_clara()` - Medoid-based clustering
- `tidy_hclust()` - Hierarchical clustering
- `tidy_dbscan()` - Density-based clustering
- `tidy_silhouette()` - Cluster validation
- `plot_clusters()` - Visualize clusters
- `quick_cluster()` - Rapid analysis with recommendations

For dimensionality reduction before clustering, see the "PCA and MDS" vignette.
For market basket analysis, see the "Market Basket Analysis" vignette.
